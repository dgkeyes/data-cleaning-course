---
title: "Common Issues in Data Cleaning"
output:
  xaringan::moon_reader:
    css: ["style.css", "default"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: 16:9
---

```{r child = "setup.Rmd"}
```

```{r xaringan-panelset, echo=FALSE, eval=TRUE}
xaringanExtra::use_panelset()
```

```{r html output, eval=TRUE, include=FALSE}
options(htmltools.dir.version = FALSE, htmltools.preserve.raw = FALSE)
```


```{r xaringan-tachyons, echo=FALSE, eval=TRUE}
xaringanExtra::use_tachyons()
```

```{r xaringan-extra-styles, eval=TRUE}
xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,     
)
```

```{r imgs setup, echo=FALSE, eval=TRUE}
knitr::opts_chunk$set(fig.retina = 3, warning = FALSE, message = FALSE)
```

```{r, include=FALSE, eval=TRUE}
library(tibble)
library(dplyr)
library(unheadr)
library(stringr)
library(janitor)
library(snakecase)
library(readr)
library(tidyr)
library(naniar)
library(gt)
```

class: center, middle, dk-section-title
background-image:url("images/michael-We2t3suGiYk-unsplash.jpg")
background-size: cover
# Common Issues in Real-World Data and Their Consequences

???

For this part of the course, we‚Äôll go over most common issues that we encounter in real-world data. The files and datasets that we receive or come across for day-to-day use are never as usable as the ones we use when we‚Äôre learning.

So, in the following lessons, we will learn how to identify these issues, and more importantly, how to fix them.

---

## üåç Real-world data

--

- May not be readily imported  


--


- May not function properly within analysis software  


--

- Often contains issues that go unnoticed until they: 

  - break a workflow
  - introduce biases
  - waste resources (computational, $)  

???

The thing with real-world data is that files and tables vary in how messy they are, and these issues can vary in how much they affect our workflows.

In some cases we might not be able to load the data into our software of choice, or maybe we can, but they will not be immediately usable and we will not be able to get things done in the first place.

Alternatively, real world data can have small issues that can go unnoticed until 
they either break something, or they introduce biases and errors. At this point it is likely that going back and fixing these issues will mean wasting time or resources.  

The good news is that most issues can be fixed before they trip up any data analysis or visualization, and simply by being aware of the most common issues we can make data usable more efficiently.

---

## General data cleaning workflows 

.large[
**0\.** Rectangular data
]

--

.large[
1\. Variable names
]

--

.large[
2\. Observational units
]

--

.large[
3\. Grouping variables
]

???

There is no single sequence for cleaning data, but I recommend this general workflow because these broad steps make our data more usable without 

The first prerequisite would be to make sure we have rectangular data, with usable rows and columns.

After that, it is advisable to have column names that mean something to us, and preferably that follow a consistent naming scheme. 

For the rows in our data, we want to be able to identify the observational units, and have one row for each observation.

Once we have usable rows and columns, then we can make sure that any variable that aggregates our observations into groups is doing so adequately.

---

## Less interpretation

.large[
Whitespace, duplication, letter case
]

--

## More interpretation

.large[
Cleaning numeric variables, unbreaking values, extracting target data
]

???

Although there is no specific recipe to follow for cleaning data, there are some steps that we can do first because they will not interfere with the rest of our work. If we recognize any of these issues in our data, we can usually fix them first because they need less interpretation from us about why these issues are present in the first place.

For example, extra spaces between words are rarely intentional and present in our data because they mean something. More likely, they are typing mistakes or artifacts from prior data transformations, so we can just get rid of any spaces that we determine to be unwanted. This can also apply to inconsistencies in letter case, and unless we are specifically interested in how capitalization varies within our data, we can usually fix this straight away.  Same with duplicate entries.

On the other hand, there are issues in our data for which we need to stop and make a more careful interpretation. For example, we might need to figure out what it means if a numeric variable has inconsistent decimal symbols or annotations, or if values are broken across more than one row or column, what criteria were used to break them. This will become clear as we go over the most common issues.

---
class: center, middle, dk-section-title
background-image:url("images/james-rathmell-t0iwmK0WC0Y-unsplash.jpg")
background-size: cover
# Unusable Headers 

???

Whether our data has a only handful of rows and columns, or millions of observations for hundreds of columns, we need ways of referring to each column. To refer efficiently to 2 or 2000 columns, we need them to have usable, informative names.

The names for the columns in our data are in the first row, or the header. If the headers have issues, we need to address them before we proceed.  This is why sorting out headers is the first general step in our workflow.

---

## Unusable headers 


Inconsistent or uninformative names  

```{r, echo=FALSE, eval=TRUE}
useless <- tibble::tribble(
  ~X,    ~X1, ~X2, ~mean_Score, ~AVERAGE.SCORE,
  "UMN", "EAST", "A",         7.7,          7.701,
  "UV", "WEST", "B",         8.9,           8.89,
  "UNLV", "EAST", "C",         9.2,          9.199
)
useless %>% 
  gt() %>% 
  tab_style(
    cell_text(size = '17px'),
    locations = list(cells_body(),
                     cells_column_labels(everything()),
                     cells_title()))
```

--


- Distinct != informative 

--

- More difficult to remember and specify  

--

- Do not sort well  

???

There are many ways for headers to be unusable. The most common issue with headers is when they have Inconsistent or uninformative names.

In this example, column names in the header are all distinct, but they do not tell us much. This name has a dot between words and this one an underscore.

Names like these are more difficult to remember, and to refer to them  in functions. Another consequence of inconsistent naming is that columns will not sort well if we want to sort them alphabetically.

Inconsistent names are fixable with string manipulation, which we will do later on.

---

## Unusable headers

.pull-left[
Names broken across rows


```{r, echo=FALSE, eval=TRUE}
tibble::tribble(
  ~X,    ~X1, ~X2,   ~mean,     ~AVERAGE,
  NA,     NA,  NA, "Score",      "SCORE",
  "UMN", "EAST", "A",   "7.7",      "7.701",
  "UV", "WEST", "B",   "8.9",       "8.89",
  "UNLV", "EAST", "C",   "9.2",      "9.199"
) %>% 
  gt(useless) %>% 
  tab_style(
    cell_text(size = '17px'),
    locations = list(cells_body(),
                     cells_column_labels(everything()),
                     cells_title()))
```
]

--

.tr[
Variable names appear in >1 rows  
]

--

.tr[
Header fragments mixed with data
]

--

.tr[
Separators become implicit  
]

--

.tr[
`NAs` introduced
]

???

Another common issue that makes headers unusable is when column names are broken across rows. This means that the header row only contains part of the column names, and the remaining parts are embedded in the first few rows which should only contain data. I‚Äôve seen this a lot when working with spreadsheets.

As we can see in this example, variable names appear outside of the header row, and fragments of the headers are mixed in data rows. The separator between fragments is nowhere to be seen, so we dont know if it was originally a space, a dot, a dash, underscore, or nothing. 

To make things worse, there are NA or empty values introduced for names that aren‚Äôt broken.

This can also fixed, but before we do so, let‚Äôs go over some good practices for naming variables.

---

## Cleaning column names

--

### Syntactically valid variable names

--

- Contain only letters, numbers, dots or underscores

--

- Start with a letter or a dot (not followed by a number)

--

- Not reserved words (_if_, _else_, _for_, _in_, _TRUE_, _NaN_, etc.)  

???

In the data organization section, we discussed good names for files, objects, and variables, but lets go a little deeper. Syntactically valid means that R will understand these names and that they will not cause any functions to fail. 

Valid names should not have special characters or reserved words meant for other uses, and start with a letter or a dot. Many of the functions that are involved in creating tables, data frames, and variables protect us from making invalid names, but we still need them to be informative and consistent.

---

## Cleaning column names 

.left-column[
</br>
.large[
Rename or clean with .orange.b[regex]
]
]
.right-column[
.large[
Clean with .b.purple[`clean_names()`] from üì¶ **.rrured.b[`janitor`]**  
]

- Strips special characters

- Changes spaces and dots to underscores

- User-defined capitalization (default is snake_case)

]

???

To clean column names, we can always perform string manipulations on the names and use regular expressions, but the JANITOR package has a useful purpose-built function  for cleaning names. 

The clean names function will strip any special characters, replace spaces and dots with underscores, and apply a consistent letter case. 


---

```{r, echo=FALSE, eval=TRUE}
badnames <- tibble::tribble(
  ~X,    ~X1, ~X2, ~mean_Score, ~AVERAGE.SCORE,
  "UMN", "EAST", "A",         7.7,          7.701,
  "UV", "WEST", "B",         8.9,           8.89,
  "UNLV", "EAST", "C",         9.2,          9.199
)
```

```{r, eval=TRUE}
badnames
```

```{r, eval=TRUE}
badnames %>% 
  clean_names()
```

???

Here, this tibble object called badnames is piped into the clean names function, with no additional arguments, and in the resulting object all the names have consistent spacing and letter case.

---

## Broken headers

```{r, echo=FALSE, eval=TRUE}
badheaders <- tibble::tribble(
  ~X,    ~X1, ~X2,   ~mean,     ~AVERAGE,
  NA,     NA,  NA, "Score",      "SCORE",
  "UMN", "EAST", "A",   "7.7",      "7.701",
  "UV", "WEST", "B",   "8.9",       "8.89",
  "UNLV", "EAST", "C",   "9.2",      "9.199"
)

```

```{r, eval=TRUE}
badheaders
```

???

Clean names make headers usable, but if they are broken across rows, we can also use a purpose-built tool to put them together. This tibble object called badheaders has its headers broken across rows

---

## Broken headers

.large[
Mash the top _n_ data rows column-wise with .b.purple[`mash_colnames()`] from üì¶ .rrured.b[`unheadr`]
]

```{r, eval=FALSE}
badheaders %>% 
    mash_colnames(n_name_rows = 1, # number of data rows with header fragments #<<
                   sep = "_") # separator for collapsing header fragments
```

> Data rows exclude the header row

???

If we encounter this issue, we can use the mash colnames function from the unheadr package. 

Mash colnames has an n name rows argument, where we specify how many data rows have name fragments, we can also specify what to use as a separator when collapsing the header fragments. 

---

```{r, eval=TRUE}
badheaders %>% 
  mash_colnames(n_name_rows = 1,
                sep = "_")
```


```{r, eval=TRUE}
badheaders %>% 
  mash_colnames(n_name_rows = 1,
                sep = "_") %>% 
  clean_names()
```

???

In this case there is one row with parts of the names, and we want to use an underscore for the new names.

After that, we can pipe the resulting object into clean names, which will give us consistent letter case.

---

class: my-turn
## My turn

.large[
- Import a .csv file with problematic headers (MPAS-mine.csv)
]

--

.large[
- Make the variable names usable by placing all header fragments in a single header row  
]

--

.large[
- Clean the names for consistency, using snake case
]

---

class: inverse

## Your turn

.large[
- Import a .csv file with problematic headers (MPAS-your.csv)
]

--

.large[
- Make the variable names usable by placing all header fragments in a single header row  
]

--

.large[
- Clean the names for consistency
]

---
class: center, middle, dk-section-title
background-image:url("images/farnoosh-abdollahi-vIkABUsLEDY-unsplash.jpg")
background-size: cover
# Whitespace

---

## Whitespace

.large[
Does not correspond to a visible character, but occupies space in a string.
]

--

```{r, eval=TRUE}
string_a <- "This string starts with a T"
string_b <- " This string starts with a space"
string_c <- "This string has trailing whitespace "
tibble(strings=c(string_a,string_b,string_c))
```

---

class: middle

```{r, eval=TRUE}
string_a <- "R for the rest of us"
string_b <- " R for the rest of us"
string_c <- "R for the rest of us "
string_d <- " R for the rest of us "
string_e <- "R for the   rest of us"
RrU <- tibble(strings=c(string_a,string_b,string_c,string_d,string_e))
RrU
```

---

```{r, eval=TRUE}
RrU %>% 
  distinct(strings)
```

---

## Issues

.large[
- Leading whitespace
]

--

.large[
- Trailing whitespace
]

--

.large[
- Duplicated whitespace within strings
]

---

## Diagnosing whitespace issues

>**"R for the rest of us"**   
(six words, five spaces)  

--

Count words, then spaces

```{r, eval=TRUE}
str_count(RrU$strings, '\\w+')
```

--

```{r, eval=TRUE}
str_count(RrU$strings, '\\s')
```

---

### Ambiguous strings

**`is.ambiguous_string()`**  

.large[
- Unexported function from üì¶ .rrured.b[`pillar`]
- Used by üì¶ .rrured.b[`tibble`] to reveal ambiguous strings when printing to console
- Ambiguous strings defined with a .orange.b[regexp]]

```{r, eval=FALSE}
function (x) 
{
  !is.na(x) & grepl("^$|^ | $|\\\\|\"", x)
}
```

---

```{r, eval=TRUE}
RrU %>% mutate(is_ambiguous = pillar:::is_ambiguous_string(strings))
```

```{r, eval=TRUE}
RrU %>% mutate(is_ambiguous = str_detect(strings,"^$|^ | $"))
```

---

.large[
Check **all** columns of type _character_
]

```{r, eval=TRUE}
RrU %>% 
  mutate(across(where(is.character), pillar:::is_ambiguous_string))
```

---

.large[
Check **all** columns of type _character_
]

```{r, eval=TRUE}
RrU %>% 
  mutate(across(where(is.character), str_detect,"^$|^ | $"))
```

---

## Removing whitespace: .b.purple[`trim_ws()`]

.large[
Argument to `read_x` functions in üì¶ .rrured.b[`readr`]
]

--

.large[
Trims leading and trailing whitespace from each field during import. 
]

--

`TRUE` by default for **`read_csv()`** and **`read_tsv()`**  

--

`FALSE` by default for **`read_delim()`**

---

## Removing whitespace: .b.purple[`str_squish()`]

String manipulation function from üì¶ **.rrured.b[`stringr`]**   

Removes whitespace from the start and end of a string and reduces repeated whitespace inside the string

--

```{r, eval=TRUE}
RrU %>% 
  mutate(strings_sq = str_squish(strings))
```

---

## Count words and spaces (again)  

.large[
**"R for the rest of us"** (six words, five spaces)
]

```{r, eval=TRUE}
str_count(RrU$strings_sq, '\\w+')
str_count(RrU$strings_sq, '\\s')
```

---

## Check ambiguous strings (again)

```{r, echo=FALSE}
RrU
```

```{r, eval=TRUE}
RrU %>% mutate(across(where(is.character), str_detect,"^$|^ | $"))
```

---

## Check ambiguous strings (again)

```{r, eval=TRUE}
RrU %>% 
  mutate(strings = str_squish(strings)) %>% 
  distinct()
```

---

class: my-turn

# My turn

.large[
- In the Marine Protected Areas dataset from the previous lesson (MPAS-mine.csv), check the _geographic_location_ variable for leading or trailing whitespace and remove it if necessary.
]

---

class: inverse

# Your turn

.large[
- In the Marine Protected Areas dataset from the previous lesson (MPAS-your.csv), check the _Country_ variable for leading or trailing whitespace and remove it if necessary.
]

---

class: center, middle, dk-section-title
background-image:url("images/snake-mylene2401.jpg")
background-size: fill
# Letter Case 

---

## Letter case

### üß∂ Strings in R are ‚å®Ô∏è case sensitive

Addressing letter case issues:

--

- Removes unwanted variation

--

- Improves noise/signal ratio

--

- Adds consistency 

--

- Improves readability (more shape contrast in lower vs. upper case, for many popular fonts)

---
```{r, echo=FALSE, eval=TRUE}
districts <- 
  tibble::tribble(
    ~District, ~`Contribution.(USD)`, ~Constituents,
    "Orange walk",                10990L,           12L,
    "TOLEDO",                30000L,            7L,
    "Stann Creek",                 3400L,            9L,
    "Toledo",                21999L,            7L,
    "Orange WalK",                 8800L,           12L,
    "Orange Walk",                  800L,           12L,
    "Orange Walk",                55000L,           12L,
    "stann creek",                22999L,            9L,
    "Toledo",                 4900L,            7L
  )
```


.panelset[
.panel[.panel-name[Districts]
```{r, echo=FALSE, eval=TRUE}
districts %>% gt() %>% 
  tab_style(
    cell_text(size = '22px'),
    locations = list(cells_body(),
                     cells_column_labels(everything()),
                     cells_title())) %>% 
  tab_options(table.background.color="#f4f4f9")
```
]

.panel[.panel-name[Data setup]
```{r panel-chunk, fig.show='hide'}
districts <- tibble::tribble(
  ~District, ~`Contribution.(USD)`, ~Constituents,
  "Orange walk",                10990L,           12L,
  "TOLEDO",                30000L,            7L,
  "Stann Creek",                 3400L,            9L,
  "Toledo",                21999L,            7L,
  "Orange WalK",                 8800L,           12L,
  "Orange Walk",                  800L,           12L,
  "Orange Walk",                55000L,           12L,
  "stann creek",                22999L,            9L,
  "Toledo",                 4900L,            7L
)
```
]
]

---

## Total contributions by district
```{r, eval=TRUE}
districts %>% 
  group_by(District) %>% 
  summarize(total_contribution = sum(`Contribution.(USD)`))
```

---

## Detecting lower and upper case

```{r, eval=TRUE}
districts %>% 
  mutate(lowercase = str_detect(District,"^[a-z\\s]+$"),
         uppercase = str_detect(District,"^[A-Z\\s]+$")) %>% 
  select(District, lowercase, uppercase)
```

---

## Changing case with üì¶ .rrured.b[`snakecase`]    

```{r, eval=FALSE}
to_any_case(strings, case)
```
--

.pull-left[
**camelCase** - capitalize all words following the first word, no spaces  

**snake_case** - all lowercase, underscores between words  

**slug-case** - all lowercase, dashes between words  
]

.pull-right[
**Title case** - All words capitalized except articles (_a_, _the_, _and_, etc.)  

**Sentence case** - First word capitalized, spaces between words  
]

---

## Convert case first with .b.purple[`to_any_case()`]


```{r, eval=TRUE}
districts %>%
  mutate(District_clean = to_any_case(District, "title",
                                      parsing_option = 0 #<<
  )) %>%  
  group_by(District_clean) %>%
  summarize(total_contribution = sum(`Contribution.(USD)`))
```

> .b.purple[`?to_any_case()`] for details on parsing options

---

## üì¶ .rrured.b[`snakecase`] shortcuts


```{r, eval=TRUE}
districts %>% 
  mutate(District_clean = to_title_case(District, #<<
                                        parsing_option = 0 #<<
  )) %>% 
  group_by(District_clean) %>%
  summarize(total_contribution = sum(`Contribution.(USD)`))
```

---

class: my-turn
## My turn

.large[
- Import the Marine Protected Areas dataset (MPAS-mine.csv) and summarize the number of Marine Protected Areas by United Nations Geoscheme Region (_UN Geoscheme_).
]

---

class: inverse
## Your turn

.large[
- Import the Marine Protected Areas dataset (MPAS-your.csv), and summarize the number of Marine Protected Areas by country (_Country full_).
]

---

class: center, middle, dk-section-title
background-image:url("images/hans-heiner-buhr-OKe4Q8azVNU-unsplash.jpg")
background-size: cover
# Missing, Implicit, or Misplaced Grouping Variables

---

### Missing ... ü§∑

---

### Implicit or misplaced

--

.large[
- Embedded as subheaders  
]

.large[
- Part of a compound value (next lesson)
]

---

## Embedded subheaders

.left-column[
**Hot drinks**
Coffee  
Tea  
Hot chocolate
**Cold Drinks**
Soda  
Juice  
Water  
Beer  
]

--

.right-column[

- Rows that correspond to values in grouping variables   

- Embedded in the data rectangle instead of in their own column   

- Human-readable, but hard to work with (for example: calculating group-wise summaries)

]

---

```{r, echo=FALSE, eval=TRUE}
cafeteria <- 
  tibble::tribble(
    ~Item, ~Price,
    "Hot Drinks",     NA,
    "Coffee",    12L,
    "Tea",     9L,
    "Hot chocolate",     9L,
    "Cold Drinks",     NA,
    "Soda",    10L,
    "Juice",    13L,
    "Water",     8L,
    "Beer",     8L
  )
cafeteria_tidy <- 
  tibble::tribble(
    ~Item, ~Price,   ~drink_type,
    "Coffee",    12L,  "Hot Drinks",
    "Tea",     9L,  "Hot Drinks",
    "Hot chocolate",     9L,  "Hot Drinks",
    "Soda",    10L, "Cold Drinks",
    "Juice",    13L, "Cold Drinks",
    "Water",     8L, "Cold Drinks",
    "Beer",     8L, "Cold Drinks"
  )

```

.panelset[
.panel[.panel-name[Menu]
```{r, echo=FALSE, eval=TRUE}
cafeteria %>% gt() %>%
  tab_style(
    cell_text(size = '25px'),
    locations = list(cells_body(),
                     cells_column_labels(everything()),
                     cells_title())) %>% 
  tab_options(table.background.color="#f4f4f9")
```
]

.panel[.panel-name[Data setup]
```{r panel-chunk2, fig.show='hide'}
cafeteria <- 
  tibble::tribble(
    ~Item, ~Price,
    "Hot Drinks",     NA,
    "Coffee",    12L,
    "Tea",     9L,
    "Hot chocolate",     9L,
    "Cold Drinks",     NA,
    "Soda",    10L,
    "Juice",    13L,
    "Water",     8L,
    "Beer",     8L
  )

```
]

.panel[.panel-name[Menu (tidy)]
```{r, echo=FALSE, eval=TRUE}
cafeteria_tidy %>% gt() %>% 
  tab_style(
    cell_text(size = '22px'),
    locations = list(cells_body(),
                     cells_column_labels(everything()),
                     cells_title())) %>% 
  tab_options(table.background.color="#f4f4f9")
```
]

.panel[.panel-name[Data setup (tidy)]
```{r panel-chunk3, fig.show='hide'}
cafeteria_tidy <- 
  tibble::tribble(
    ~Item, ~Price,   ~drink_type,
    "Coffee",    12L,  "Hot Drinks",
    "Tea",     9L,  "Hot Drinks",
    "Hot chocolate",     9L,  "Hot Drinks",
    "Soda",    10L, "Cold Drinks",
    "Juice",    13L, "Cold Drinks",
    "Water",     8L, "Cold Drinks",
    "Beer",     8L, "Cold Drinks"
  )

```
]
]

---

## Tidying embedded subheaders
.large[
.b.purple[`untangle2()`] from üì¶ .rrured.b[`unheadr`]
]

--

1. Match with a .orange.b[regular expression]

--

2. Position (sometimes) identifiable as the rows with NAs in all the variables except for the one with the subheaders

---

## `untangle2()`

```{r, echo=FALSE, eval=TRUE}
cafeteria
```

--

```{r, eval= FALSE}
untangle2(df,
          regex, # regular expression to match subheaders
          orig,  # variable with the subheaders
          new)   # name of the new variable with the group values
```

---

## `untangle2()`

Match using .orange.b[regex]
```{r, eval= TRUE}
cafeteria %>% 
  untangle2("Drinks$", Item, drink_type) %>% #<<
  clean_names()
```

---

## `untangle2()`

.orange.b[Regexp] match and summarize
```{r, eval=TRUE}
cafeteria %>% 
  untangle2("Drinks$", Item, drink_type) %>% 
  clean_names()
```


```{r, eval=TRUE}
cafeteria %>% 
  untangle2("Drinks$", Item, drink_type) %>% 
  clean_names() %>% 
  group_by(drink_type) %>% 
  summarize(mean_price=mean(price))
```

---

### Alternation

```{r, eval=TRUE}
cafeteria %>% 
  untangle2("Hot Drinks|Cold Drinks", Item, drink_type) %>% 
  clean_names()
```

---

### Alternation

```{r, eval=TRUE}
cafeteria %>% 
  untangle2("Hot Drinks|Cold Drinks", Item, drink_type) %>% 
  clean_names() %>% 
  group_by(drink_type) %>% 
  summarize(mean_price=mean(price))
```

---

### Identify subheaders by context (`NA` values)

```{r, eval=TRUE}
cafeteria %>%
  filter(across(-Item, is.na)) %>%
  select(Item) %>%
  mutate(tag = paste0("subheader_", Item)) %>% 
  right_join(cafeteria) %>%
  untangle2("^subheader", tag, drink_type) %>%
  select(-tag) %>%
  mutate(drink_type = stringr::str_remove(drink_type, "^subheader_"))
```

---


```{r, eval=TRUE}
cafeteria %>%
  filter(across(-Item, is.na)) %>%
  select(Item) %>%
  mutate(tag = paste0("subheader_", Item)) 
```


--

Filter rows with NAs in all columns except the one holding the subheaders  

--

Keep only the column with subheaders  

--

Create a new variable that includes a pattern that can be matched with .orange.b[regex]


---


```{r, eval=FALSE}
... %>%    
  right_join(cafeteria) %>%
  untangle2("^subheader", tag, drink_type) %>%
  select(-tag) %>%
  mutate(drink_type = stringr::str_remove(drink_type, "^subheader_"))
```

--

Join resulting object with original data

--

Put implicit grouping variable into its own column

--

Clean up resulting data


---

class: my-turn
# My turn
.large[
- Load the `primates2017` dataset bundled with üì¶ `unheadr` and create a new column that groups the different species by geographic region. 
]

---

class: inverse
# Your turn
.large[
- Load the `primates2017` dataset bundled with üì¶ `unheadr` and create a new column that groups the different species by taxonomic family.  
]

> In biology, taxonomic families all end in the suffix "_DAE_"  

.large[- How many different ways can you identify the embedded subheaders in these data?
]

---

class: center, middle, dk-section-title
background-image:url("images/max-vertsanov-qvRuue12Huw-unsplash.jpg")
background-size: fill
# Compound Values  

---

## Compound values

.large[
* Not tidy
]  

--

.large[
* Potential loss of variables or observations
]

---

### Tidying compound values 

.pull-left[Split on a delimiter, then:

**`separate()`** columns  

**`separate_rows()`**
]

.pull-right[
Match with .orange.b[regex], then:  

**`str_extract()`** substrings into new columns
]

---

```{r, include=FALSE, eval=TRUE}
households <- 
  tibble::tribble(
    ~Participant.ID,     ~Education,            ~Residence, ~Bonus,
    "GHC21",   "Vocational",    "Living Alone-Cat",   500L,
    "MYL11",  "High School",     "Family Home-Cat",   400L,
    "LLB16",     "Graduate",     "Family Home-Dog",   400L,
    "AAH08",   "Vocational", "Shared Housing-None",   450L,
    "PCG91",   "Vocational",   "Family Home-Other",   500L,
    "ACC22, PMM02",  "High School", "Shared Housing-None",   400L,
    "MJM13", "Postgraduate",    "Living Alone-Dog",   500L
  )
```


.panelset[
.panel[.panel-name[Households]
```{r echo=FALSE, eval=TRUE}
gt(households)
```
]

.panel[.panel-name[Data setup]
```{r householdsetup, fig.show='hide'}
households <- 
  tibble::tribble(
    ~Participant.ID,     ~Education,            ~Residence, ~Bonus,
    "GHC21",   "Vocational",    "Living Alone-Cat",   500L,
    "MYL11",  "High School",     "Family Home-Cat",   400L,
    "LLB16",     "Graduate",     "Family Home-Dog",   400L,
    "AAH08",   "Vocational", "Shared Housing-None",   450L,
    "PCG91",   "Vocational",   "Family Home-Other",   500L,
    "ACC22, PMM02",  "High School", "Shared Housing-None",   400L,
    "MJM13", "Postgraduate",    "Living Alone-Dog",   500L
  )
```
]
]

---

## Separate Residence and 'Pet' variables

.large[
.b.purple[`separate_rows()`] from üì¶ .rrured.b[`tidyr`] 
]

.large[
- Turns a single character column into multiple columns
]

--

_Arguments_  

.b[`sep`]"  what separates values that should not be together in a single column  

.b[`into`]: names for the new columns to create

---

## Separate Residence and 'Pet' variables

```{r, eval=TRUE}
households %>% 
  separate(col = Residence, # columns to separate
           into = c("Residence", "Pet"), # names of new variables to create 
           sep = "-")   # separator between columns
```

---

### Match and extract
.large[
.orange.b[Regexp] for ~'last word in string'
]
```{r, eval=TRUE}
households %>% 
  mutate(Pet = str_extract(Residence,"\\b\\w+$"))
```

---

## Separate Participant IDs

.large[
.b.purple[`separate_rows()`] from üì¶ .rrured.b[`tidyr`] 
- Separates multiple delimited observations within a column and places each one in its own row.]

---

## Separate Participant IDs

```{r, eval=TRUE}
households %>% 
  separate_rows(Participant.ID,
                sep = ", ") #<<  

```

---

class: my-turn
## My turn

.large[
- Import the Marine Protected Areas dataset (MPAS-mine.csv), separate the country codes variable (ISO3 and UN scheme) into columns the Reference variable into rows.]

--

.large[
- Which Reference appears the most?
]

---

class: inverse
## Your turn

.large[
- Import the Marine Protected Areas dataset (MPAS-your.csv), separate the country codes variable (ISO3 and UN scheme) into columns and the Reference variable into rows.]

--

> Keep an eye on the separators

--

.large[
- Optional: Arrange the data by ISO3 country code
]

---

class: center, middle, dk-section-title
background-image:url("images/armelle-danjour-kwGqIuNrM5E-unsplash.jpg")
background-size: cover
# Duplicates  

---

## Problems with duplicates

--

.large[
- Bloat our data
]  

--

.large[
- Unintentional repetition can be costly
]  

--

.large[
- Inaccurate reporting (double counting, inflated or biased summary statistics)
]

---

## What can we do?

.large[
Identify with .b.purple[`get_dupes()`] from üì¶ .rrured.b[`janitor`]  
]

--

.large[
Discard with .b.purple[`distinct()`] from üì¶ .rrured.b[`dplyr`]  
]

---

## Duplicated values  

.large[
- Across all variables  
]

--

.large[
- Across the variable(s) defining the observational units  
]

--

.large[
- Across arbitrary sets of variables
]

---

```{r, include=FALSE, eval=TRUE}
pizza_orders <- 
  tibble::tribble(
  ~CustomerID,                             ~Address,           ~City,     ~State,
     "Newman", "Apartment 5E, 129 West 81st Street", "New York City", "New York",
    "susan_A",               "185 West 74th Street", "New York City", "New York",
    "susan_A",               "185 West 74th Street", "New York City", "New York",
     "js1994", "Apartment 5A, 129 West 81st Street", "New York City", "New York",
       "Eric",    "Unit 1B, 4186 Willoughby Avenue",      "Brooklyn", "New York",
       "Dash",    "Unit 1B, 4186 Willoughby Avenue",      "Brooklyn", "New York",
     "Rakeem",    "Unit 1B, 4186 Willoughby Avenue",      "Brooklyn", "New York"
  )
```


.panelset[
.panel[.panel-name[pizza_orders]
```{r, echo=FALSE, eval=TRUE}
pizza_orders %>% gt() %>% 
    tab_style(
    cell_text(size = '26px'),
    locations = list(cells_body(),
                     cells_column_labels(everything()),
                     cells_title()))
  
```
]

.panel[.panel-name[Data setup]
```{r pizzasetup, fig.show='hide'}
pizza_orders <- 
  tibble::tribble(
  ~CustomerID,                             ~Address,           ~City,     ~State,
     "Newman", "Apartment 5E, 129 West 81st Street", "New York City", "New York",
    "susan_A",               "185 West 74th Street", "New York City", "New York",
    "susan_A",               "185 West 74th Street", "New York City", "New York",
     "js1994", "Apartment 5A, 129 West 81st Street", "New York City", "New York",
       "Eric",    "Unit 1B, 4186 Willoughby Avenue",      "Brooklyn", "New York",
       "Dash",    "Unit 1B, 4186 Willoughby Avenue",      "Brooklyn", "New York",
     "Rakeem",    "Unit 1B, 4186 Willoughby Avenue",      "Brooklyn", "New York"
  )

```
]
]

---

.large[
Identify with .b.purple[`get_dupes()`] from üì¶ .rrured.b[`janitor`]  
]

```{r, eval=TRUE}
pizza_orders %>% 
  get_dupes() # all variables by default
```

--

* Adds a `dupe_count` variable showing the number of rows sharing the duplicated values  
* Puts the input variables at the beginning of the resulting data frame 

---

.large[
.b.purple[`get_dupes()`] using variable with chosen observational unit
]

```{r, eval=TRUE}
pizza_orders %>% 
  get_dupes(CustomerID)
```

---

.large[
.b.purple[`get_dupes()`] with an arbirary set of variables
]
> now accepts üì¶ .rrured.b[`tidyselect`] helpers

```{r, eval=TRUE}
pizza_orders %>% 
  get_dupes(Address, starts_with("Cit")) 
```

---

.large[
Discard with .b.purple[`distinct()`] from üì¶ .rrured.b[`dplyr`]
]

```{r, eval=TRUE}
pizza_orders %>% 
  distinct() # all variables by default
```

---

.large[
.b.purple[`distinct()`] using variable with the chosen observational unit
]

```{r, eval=TRUE}
pizza_orders %>% 
  distinct(CustomerID, .keep_all = TRUE)
```

--

> Use `.keep_all = FALSE` (default) to drop all other variables

---

.large[
.b.purple[`distinct()`] with custom combination of variables
]


>  üì¶ .b.rrured[`tidyselect`] helpers enabled by using .b[`across`] semantics

```{r, eval=TRUE}
pizza_orders %>% 
  distinct(across(c(Address, starts_with("Cit")))) 
```

---

.large[
.b.purple[`distinct()`] with custom combination of variables
]

>‚ö†Ô∏è If `.keep_all = TRUE` and there are duplicates in other variables, `distinct` only keeps the first row 

```{r, eval=TRUE}
pizza_orders %>% 
  distinct(across(c(Address,starts_with("City"))), .keep_all = TRUE) 
```

---

class: my-turn

# My turn

--

.large[
- Load the messy Age of Empires units dataset bundled with `unheadr` (AOEunits_raw) and discard units that are not of Type "Archer".
]

--

.large[
- Identify duplicated records across all variables.
]

--

.large[
- Remove duplicated records across all variables.
]

---

class: inverse
# Your turn

.large[
- Load the messy Age of Empires units dataset bundled with `unheadr` (AOEunits_raw) and keep only units of Type "Cavalry".
]

--

.large[
- Identify duplicated records across all variables.
]

--

.large[
- Remove duplicated records across all variables.
]

---

class: center, middle, dk-section-title
background-image:url("images/jonathan-safa-YcxOAC5DpDA-unsplash.jpg")
background-size: cover
# Broken Values

---

# Broken values

.large[
Values broken across rows, often to save horizontal space
]

--

.large[
`NA` or blank values introduced  
]

--

.large[
Problematic when grouping variables or observational units are broken
]

---

```{r, echo=FALSE, eval=TRUE}
olympics <- 
  tibble::tribble(
    ~Edition,    ~Country, ~`Soccer.gold.medal.(men)`, ~`Wrestling.(men.middleweight)`,
    NA,          NA,                         NA,           "weight class limit:",
    NA,          NA,                         NA,                         "82 kg",
    "Los Angeles 1984",       "USA",                   "France",                           "USA",
    "Barcelona",     "Spain",                    "Spain",                           "USA",
    "1992",          NA,                         NA,                              NA,
    "Atlanta 1996",       "USA",                  "Nigeria",                        "Russia",
    NA,          NA,                         NA,           "weight class limit:",
    NA,          NA,                         NA,                         "85 kg",
    "Sydney 2000", "Australia",                 "Cameroon",                        "Russia",
    "London",        "UK",                   "Mexico",                    "Azerbaijan",
    "2012",          NA,                         NA,                              NA
  )

```


.panelset[
.panel[.panel-name[olympics]
```{r olympicsgt, echo=FALSE, eval=TRUE}
gt(olympics) %>% 
  tab_style(
    cell_text(size = '20px'),
    locations = list(cells_body(),
                     cells_column_labels(everything()),
                     cells_title()))
```
]

.panel[.panel-name[Data setup]
```{r olympics_setup, fig.show='hide'}
olympics <- 
  tibble::tribble(
    ~Edition,    ~Country, ~`Soccer.gold.medal.(men)`, ~`Wrestling.(men.middleweight)`,
    NA,          NA,                         NA,           "weight class limit:",
    NA,          NA,                         NA,                         "82 kg",
    "Los Angeles 1984",       "USA",                   "France",                           "USA",
    "Barcelona",     "Spain",                    "Spain",                           "USA",
    "1992",          NA,                         NA,                              NA,
    "Atlanta 1996",       "USA",                  "Nigeria",                        "Russia",
    NA,          NA,                         NA,           "weight class limit:",
    NA,          NA,                         NA,                         "85 kg",
    "Sydney 2000", "Australia",                 "Cameroon",                        "Russia",
    "London",        "UK",                   "Mexico",                    "Azerbaijan",
    "2012",          NA,                         NA,                              NA
  )
```
]
]

---

## 'Unbreaking' values 

--

.large[
For values broken up across **two consecutive rows**:
]

--

.large[
Match the trailing or leading half of the value with a .orange.b[regexp] and üì¶ .rrured.b[`unheadr`]
]
---

## 'Unbreaking' values 

.pull-left[
Fast  
food  
Casual dining  
Thai  
Pizzeria  
Cakes and  
ice cream  
]  

.pull-right[
Retriever  
(flat-coated)  
Bulldog (American)   
Bullmastiff  
Retriever  
(golden)  
Poodle  
]

---

## 'Unbreaking' values

.pull-left[
Fast  
**food**  
Casual dining  
Thai  
Pizzeria  
Cakes and  
**ice cream**  
]  

.pull-right[
Retriever  
**(flat-coated)**  
Bulldog (American)   
Bullmastiff  
Retriever  
**(golden)**  
Poodle  
]

---

## .purple[**`unbreak_vals()`**]  

.large[
verb-like function from üì¶ .rrured.b[`unheadr`]

Match the trailing half of the broken value with a .orange.b[regular expression]
]
```{r, eval=FALSE}
unbreak_vals(df, 
             regex, 
             ogcol, 
             newcol, 
             sep = " ",
)
```

---

## Trailing row


```{r, eval=TRUE}
olympics %>% 
  unbreak_vals(regex = "^\\d", # numbers at start of string
               ogcol = Edition,
               newcol = Edition_ub,
               sep = " ")
```

---

### .purple[**`unbreak_rows()`**]  


Verb for merging rows from üì¶ .rrured.b[`unheadr`]

Match the leading half of the broken value with .orange.b[regular expression]


```{r, eval=FALSE}
unbreak_rows(df, 
             regex, 
             ogcol, 
             sep = " ")
```

---

## Leading row

```{r, eval=TRUE}
olympics %>% unbreak_rows(regex = "^weight", # numbers at start of string
                          ogcol = `Wrestling.(men.middleweight)`,
                          sep = " ")
```

---

## Leading and trailing rows

```{r, eval=TRUE}
olympics %>%
  unbreak_rows(
    regex = "^weight",
    ogcol = `Wrestling.(men.middleweight)`) %>%
  unbreak_vals(regex = "^\\d", # numbers at start of string
               ogcol = Edition,
               newcol = Edition_ub)
```

---

class: my-turn

# My turn

--

.large[
- Load the messy Age of Empires units dataset from csv (aoe_raw.csv)
]

--

.large[
- Identify the broken values in the 'Type' column and unbreak them
]

---

class: inverse
# Your turn

--

.large[
- Load the messy Age of Empires units dataset from csv (aoe_raw.csv)
]

--

.large[
- Identify the broken values in both the 'Type' and 'Name' columns and unbreak them
]

--

.large[
- Clean up any separator-related issues arising from the 'unbreaking'
]

---

class: center, middle, dk-section-title
background-image:url("images/david-hertle-8HAhmMk9HJI-unsplash.jpg")
background-size: cover

# Empty Rows and Columns

---

### Problems with empty rows and columns

--
.large[
- Common when importing files
]

--

.large[
- Problematic when referring to variables by position or ranges of consecutive variables  (`:`)  
]

--

.large[
- Potentially disruptive when filling adjacent cells
]

---

# What can we do?

--

.large[
**Identify** with üì¶ .rrured.b[`dplyr`]  
]

--

.large[
**Discard** with .purple[**`remove_empty()`**] from üì¶ .rrured.b[`janitor`]
]

---

```{r, echo=FALSE, eval=TRUE}
universities <- 
  tibble::tribble(
    ~ID,                          ~Institution, ~year,    ~ZIP.code, ~Highest.degree.offered, ~County.name, ~Religious.affiliation,
    NA,                                    NA,    NA,           NA,                      NA,           NA,                     NA,
    100663L, "University of Alabama at Birmingham",    NA, "35294-0110",       "Doctor's degree",           NA,       "Not applicable",
    100690L,                  "Amridge University",    NA, "36117-3553",       "Doctor's degree",           NA,   "Churches of Christ",
    100706L, "University of Alabama in Huntsville",    NA,      "35899",       "Doctor's degree",           NA,       "Not applicable",
    NA,                                    NA,    NA,           NA,                      NA,           NA,                     NA,
    100751L,           "The University of Alabama",    NA, "35487-0166",       "Doctor's degree",           NA,       "Not applicable",
    NA,                                    NA,    NA,           NA,                      NA,           NA,                     NA,
    NA,                                    NA,    NA,           NA,                      NA,           NA,                     NA,
    101541L,                      "Judson College",    NA,      "36756",     "Bachelor's degree",           NA,              "Baptist",
    101587L,          "University of West Alabama",    NA,      "35470",       "Master's degree",           NA,       "Not applicable",
    NA,                                    NA,    NA,           NA,                      NA,           NA,                     NA,
    101693L,                "University of Mobile",    NA, "36613-2842",       "Master's degree",           NA,     "Southern Baptist"
  )

```

.panelset[
.panel[.panel-name[universities]
```{r, echo=FALSE, eval=TRUE}
universities%>% gt() %>% 
  tab_style(
    cell_text(size = '14px'),
    locations = list(cells_body(),
                     cells_column_labels(everything()),
                     cells_title())) %>%  
  tab_options(table.background.color="#f4f4f9")
```
]

.panel[.panel-name[Data setup]
```{r unissetup, fig.show='hide'}
universities <- 
  tibble::tribble(
    ~ID,                          ~Institution, ~year,    ~ZIP.code, ~Highest.degree.offered, ~County.name, ~Religious.affiliation,
    NA,                                    NA,    NA,           NA,                      NA,           NA,                     NA,
    100663L, "University of Alabama at Birmingham",    NA, "35294-0110",       "Doctor's degree",           NA,       "Not applicable",
    100690L,                  "Amridge University",    NA, "36117-3553",       "Doctor's degree",           NA,   "Churches of Christ",
    100706L, "University of Alabama in Huntsville",    NA,      "35899",       "Doctor's degree",           NA,       "Not applicable",
    NA,                                    NA,    NA,           NA,                      NA,           NA,                     NA,
    100751L,           "The University of Alabama",    NA, "35487-0166",       "Doctor's degree",           NA,       "Not applicable",
    NA,                                    NA,    NA,           NA,                      NA,           NA,                     NA,
    NA,                                    NA,    NA,           NA,                      NA,           NA,                     NA,
    101541L,                      "Judson College",    NA,      "36756",     "Bachelor's degree",           NA,              "Baptist",
    101587L,          "University of West Alabama",    NA,      "35470",       "Master's degree",           NA,       "Not applicable",
    NA,                                    NA,    NA,           NA,                      NA,           NA,                     NA,
    101693L,                "University of Mobile",    NA, "36613-2842",       "Master's degree",           NA,     "Southern Baptist"
  )
```
]
]
.small[source: US Integrated Postsecondary Education Data System (IPEDS)]

---


## Identify empty rows with üì¶ .rrured.b[`dplyr`] (and üì¶ .rrured.b[`tibble`])  

```{r, eval=TRUE}
universities %>% filter(across(everything(), is.na))
universities %>% filter(across(everything(), is.na)) %>% nrow()
```

---

### Add unique identifier first

```{r, eval=TRUE}
universities %>% 
  rowid_to_column() %>% #<<
  filter(across(-rowid, is.na)) 
```

---

### Negate the condition .b.purple[(!)] and discard empty rows

```{r, eval=TRUE}
universities %>% 
  filter(!across(everything(), is.na))
```

---

.large[
Discard empty rows with .purple.b[`remove_empty()`]
]  

> `quiet = FALSE` prints a statement about the rows that were removed

```{r message=TRUE, eval=TRUE}
universities %>% 
  remove_empty(which = "rows",
               quiet = FALSE) 
```

---

## Empty columns 

.large[
Identify with üì¶ .rrured.b[`dplyr`] and .b.purple[`all_na()`] from üì¶ .rrured.b[`naniar`]  
]

```{r, eval=TRUE}
universities %>% 
  select(where(all_na))
```

---

### Identify and get names

```{r, eval=TRUE}
universities %>% 
  select(where(all_na)) %>% 
  names()
```

---

### Negate condition for selection, get names


```{r, eval=TRUE}
universities %>% 
  select(!where(all_na)) %>% 
  names()
```

---

### Discard empty columns with .b.purple[`remove_empty()`] 


```{r, eval=TRUE, message=TRUE}
universities %>% 
  remove_empty(which= "cols", quiet = FALSE) 
```


---

class: my-turn

# My turn

--

- Import the Marine Protected Areas dataset (MPAS-mine.csv)

--

- Identify the empty rows and columns, and create a new object with only the empty rows and columns 

--

- Remove the empty rows and columns 


---
class: inverse

# Your turn

--

- Import the Marine Protected Areas dataset (MPAS-your.csv)

--

- Identify the empty rows and columns, and create a new object with only the empty rows and columns 

--

- Remove the empty rows and columns

--

---

class: center, middle, dk-section-title
background-image:url("images/markus-spiske-4jiR57y3jgY-unsplash.jpg")
background-size: cover
# Parsing Numbers

---
class: middle

.large[
- Vectors must have all their values of the same mode (_character, numeric, logical_)]

--

.large[
- If there is a character string is present in a vector, everything else in the vector will be converted to character strings
]

---

### We cannot perform arithmetic operations on:

--

- Numbers stored as text

--

- Strings with non-digits

--

```{r message=TRUE, warning=TRUE, eval=TRUE}
test_scores <- c(8.8,9,10,7.2,8.4)
class(test_scores)
class(c(test_scores,"a"))
```

---

## Numbers stored as text


1. .b.purple[`type_convert()`] the whole object (üì¶ .rrured.b[`readr`])

--

2. Coerce one or more variables to `numeric` with üì¶ .rrured.b[`dplyr`]


---

## Strings with non-digits


1. Parse with üì¶ .rrured.b[`readr`]  

2. Clean with .orange.b[regular expressions]


---
```{r, include=FALSE, eval=TRUE}
burger_prices <- 
  tibble::tribble(
    ~Rank,        ~Country,   ~Price, ~Price.ARS,   ~Region,
    "1",   "Switzerland",    "6.8",  "$544.50",  "Europe",
    "2",        "Norway",    "6.2",      "496",  "Europe",
    "3",        "Sweden",    "6.1",      "488",  "Europe",
    "4",       "Finland",    "5.6",      "448",  "Europe",
    "5", "United States", "5.3[1]",  "$424.50", "America",
    "8",         "Italy",    "5.1",      "408",  "Europe",
    "7",        "France",    "5.1",   "408,50",  "Europe",
    "6",        "Canada",    "5.3",      "424", "America",
    "9",        "Brazil",   "5.1*",      "408", "America"
  )

```

.panelset[
.panel[.panel-name[Burger Prices]
```{r, echo=FALSE, eval=TRUE}
burger_prices %>% gt() %>%
  tab_style(
    cell_text(size = '24px'),
    locations = list(cells_body(),
                     cells_column_labels(everything()),
                     cells_title())) %>% 
  tab_options(table.background.color="#f4f4f9",container.overflow.x = TRUE, container.overflow.y = TRUE)
```
]

.panel[.panel-name[Data setup]
```{r burgerssetup, fig.show='hide'}
burger_prices <- 
  tibble::tribble(
    ~Rank,        ~Country,   ~Price, ~Price.ARS,   ~Region,
    "1",   "Switzerland",    "6.8",  "$544.50",  "Europe",
    "2",        "Norway",    "6.2",      "496",  "Europe",
    "3",        "Sweden",    "6.1",      "488",  "Europe",
    "4",       "Finland",    "5.6",      "448",  "Europe",
    "5", "United States", "5.3[1]",  "$424.50", "America",
    "8",         "Italy",    "5.1",      "408",  "Europe",
    "7",        "France",    "5.1",   "408,50",  "Europe",
    "6",        "Canada",    "5.3",      "424", "America",
    "9",        "Brazil",   "5.1*",      "408", "America"
  )
```
]
]

---

## Numbers stored as text  

Parse all columns

```{r, eval=TRUE}
burger_prices %>% 
  type_convert()
```

---

## Numbers stored as text  

```{r, eval=TRUE}
burger_prices 
```

---

## Numbers stored as text  

Coerce variables to `numeric`

```{r, eval=TRUE}
burger_prices %>% 
  mutate(Rank = as.numeric(Rank))
```

---

## Strings with non-digits

Parse with üì¶ .rrured.b[`readr`] (‚ö† watch out for inconsistent decimals)

```{r, eval=TRUE}
burger_prices %>% 
  mutate(across(c(Rank, Price, Price.ARS), parse_number))
```

---

## Strings with non-digits

.large[
Clean with .orange.b[regular expressions] then .b.purple[`type_convert()`]
]

```{r, echo=FALSE, eval=TRUE}
options(pillar.sigfig=5)
```


```{r, eval=FALSE}
burger_prices %>%
  mutate(across(c(Rank, Price, Price.ARS), str_remove_all, "\\[.+\\]|\\(.+\\)")) %>%
  mutate(across(c(Rank, Price, Price.ARS), str_remove_all, "[^0-9.,]")) %>%
  mutate(Price.ARS = str_replace(Price.ARS, ",", ".")) %>% 
  type_convert()
```

---

```{r, echo=FALSE, eval=TRUE}
options(pillar.sigfig=5)
```

```{r, eval=TRUE}
burger_prices %>%
  mutate(across(c(Rank, Price, Price.ARS), str_remove_all, "\\[.+\\]|\\(.+\\)")) %>%
  mutate(across(c(Rank, Price, Price.ARS), str_remove_all, "[^0-9.,]")) %>%
  mutate(Price.ARS = str_replace(Price.ARS, ",", ".")) %>% 
  type_convert()
```

---
class: my-turn
# My turn

--
.large[
- Import the Marine Protected Areas dataset (MPAS-mine.csv)
]

--

.large[
- Make the columns that hold the MPA extent into usable numeric variables
]

---
class: inverse
# Your turn

--

.large[
- Import the Marine Protected Areas dataset (MPAS-mine.csv)
]

--

.large[
- Subset to keep only the MPA names and columns with extent data
]

--

.large[
- Make the columns that hold the MPA extent into usable numeric variables (Watch out for the decimals!)]


---

class: center, middle, dk-section-title
background-image:url("images/ryan-quintal-US9Tc9pKNBU-unsplash.jpg")
background-size: cover


# Putting Everything Together

---

## Demonstration

.large[Watch me load and wrangle the messy Age of Empires units dataset into a usable, tidy object]